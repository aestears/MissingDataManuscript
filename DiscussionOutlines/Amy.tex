Main takeaways that I've gotten from what we have so far (still missing prediction results):
\begin{itemize}
    \item Autocorrelation in which data are missing will generally not be a big problem for parameter estimation unless these stretches coincide with particular conditions in the covariates (aka a type of nonrandom missingness). In fact, increased autocorrelation in missing data may be better, because having longer uninterrupted stretches of complete data is better than many smaller stretches of data. Researcher should keep this in mind when designing experiments and anticipating what situations could cause missing data.
    \item It is really important to match your data type with the missing data method that you use. Something useful here would be to consider what the autocorrelation structure might be in a certain time series. If it population data, that might look like Ricker data, where each data point only relies on the previous time point, then complete case data deletion can be one of the best methods, whereas if you have an autoregressive process that has several covariates, something like the Kalman filter might be the best choice.
    \item Data deletion can be a really poorly performing method (second worst for Gaussian, worst for Poisson), especially if it is done blindly. The exception is Ricker data with the complete case method which actually works the best! However simple data deletion is always one of the worst methods- just never do this!
    \item The multiple imputation method is just not very good, and probably not worth your time (worst for Gaussian, second worst for Poisson)
    \item There are many different missing data methods that performed similarly in our numerical experiments. The most complex method won't necessarily net you any improvement.
    \item For the Gaussian data, data that is missing not at random can be a big problem for all of the methods that we tested, often producing very biased estimates with the bias increasing with the amount of data that is missing. Data that is missing at random can often produce decently un-biased estimates, but the confidence in an estimate will decrease with increased missingness. Researchers should consider how much bias they are willing to risk when deciding how much missing data is too much.
    \item Ricker time series estimates can be especially biased due to low sample size- this is a common problem that estimators struggle at small sample sizes. Unfortunately, population data sets are usually small, so researchers need to keep this in mind when using GLMs to estimate Ricker model parameters.
\end{itemize}

Missing data is a big problem for modern analyses on real data sets, and there are some excellent solutions available that enable a researcher to run models and obtain results even with missing data. Especially if a researcher is careful in choosing an appropriate method for their data type, and doesn't have a high proportion of missing data, very good results are obtainable using a few different methods. However, there are some conditions that no out-of-the-box method can address. This includes non-random missingness, and we would like to caution researchers with this type of data for now. Eventually, further understanding of what causes non-random missingness could allow researchers to make a model that will give better results- maybe Bayesian methods with changeable priors could help with this, but this kind of work will be system specific, and likely time consuming. 
